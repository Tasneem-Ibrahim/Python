{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tasneem-Ibrahim/Python/blob/main/Project_03_Function_calling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDS9Xcj_8k-T"
      },
      "source": [
        "# Function_calling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Function Calling Config"
      ],
      "metadata": {
        "id": "rZfOTN-qvfbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Google Generative AI Library"
      ],
      "metadata": {
        "id": "lXOGXSyGyUrD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4DhA4907Asz"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q \"google-generativeai>=0.7.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wp3W4Pdf8rBO"
      },
      "outputs": [],
      "source": [
        "# Import Necessary Modules\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configure Google Generative AI API\n",
        "genai.configure(api_key=userdata.get(\"GOOGLE_API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJqil-VL8ug-"
      },
      "source": [
        "### Set up a model with tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLS26n7A9l9B"
      },
      "outputs": [],
      "source": [
        "def turn_on_uv_lights():\n",
        "    \"\"\"Turn on the uv lighting system.\"\"\"\n",
        "    print(\"LIGHTBOT: UV lights enabled.\")\n",
        "\n",
        "def turn_on_light():\n",
        "    \"\"\"Turn on the lighting system.\"\"\"\n",
        "    print(\"LIGHTBOT: Lights enabled.\")\n",
        "\n",
        "\n",
        "def change_light_color(rgb_hex: str):\n",
        "    \"\"\"Set the light color. Lights must be enabled for this to work.\"\"\"\n",
        "    print(f\"LIGHTBOT: Lights set to {rgb_hex}.\")\n",
        "\n",
        "\n",
        "def turn_off_lights():\n",
        "    \"\"\"Stop flashing lights.\"\"\"\n",
        "    print(\"LIGHTBOT: Lights turned off.\")\n",
        "\n",
        "\n",
        "light_controls = [turn_on_uv_lights, turn_on_light, change_light_color, turn_off_lights]\n",
        "instruction = \"You are a helpful lighting system bot. You can turn lights on and off, and you can set the color. Do not perform any other tasks.\"\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    \"models/gemini-1.5-pro\", tools=light_controls, system_instruction=instruction\n",
        ")\n",
        "\n",
        "chat = model.start_chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqROCznFCj_Y"
      },
      "source": [
        "Create a helper function for setting `function_calling_config` on `tool_config`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QgLFPL4Chon"
      },
      "outputs": [],
      "source": [
        "from google.generativeai.types import content_types\n",
        "from collections.abc import Iterable\n",
        "\n",
        "\n",
        "def tool_config_from_mode(mode: str, fns: Iterable[str] = ()):\n",
        "    \"\"\"Create a tool config with the specified function calling mode.\"\"\"\n",
        "    return content_types.to_tool_config(\n",
        "        {\"function_calling_config\": {\"mode\": mode, \"allowed_function_names\": fns}}\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofMEuh_MFdMf"
      },
      "source": [
        "### Text-only mode: `NONE`\n",
        "\n",
        "If you have provided the model with tools, but do not want to use those tools for the current conversational turn, then specify `NONE` as the mode. `NONE` tells the model not to make any function calls, and will behave as though none have been provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZlIFwXqGA09",
        "outputId": "02564832-b6c7-4744-aa33-9784d84dcdeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I can turn lights on and off, and I can change their color.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tool_config = tool_config_from_mode(\"none\")\n",
        "\n",
        "response = chat.send_message(\n",
        "    \"Hello light-bot, what can you do?\", tool_config=tool_config\n",
        ")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uux063sjHZ_Z"
      },
      "source": [
        "### Automatic mode: `AUTO`\n",
        "\n",
        "To allow the model to decide whether to respond in text or call specific functions, you can specify `AUTO` as the mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwO9dUjvHoT8",
        "outputId": "e5772234-40a3-4306-e9d0-a97d845bc084",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "function_call {\n",
            "  name: \"turn_on_uv_lights\"\n",
            "  args {\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tool_config = tool_config_from_mode(\"auto\")\n",
        "\n",
        "response = chat.send_message(\"Turn on the UV lights!\", tool_config=tool_config)\n",
        "print(response.parts[0])\n",
        "chat.rewind();  # You are not actually calling the function, so remove this from the history."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHhaO-P9CBPb"
      },
      "source": [
        "### Function-calling mode: `ANY`\n",
        "\n",
        "Setting the mode to `ANY` will force the model to make a function call. By setting `allowed_function_names`, the model will only choose from those functions. If it is not set, all of the functions in `tools` are candidates for function calling.\n",
        "\n",
        "In this example system, if the lights are already on, then the user can change color or turn the lights off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQpz94zrCNJF",
        "outputId": "9fa749c2-7fe5-488c-ee02-fa156d7d8f12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "function_call {\n",
            "  name: \"change_light_color\"\n",
            "  args {\n",
            "    fields {\n",
            "      key: \"rgb_hex\"\n",
            "      value {\n",
            "        string_value: \"FF6600\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "available_fns = [\"change_light_color\", \"turn_off_lights\"]\n",
        "\n",
        "tool_config = tool_config_from_mode(\"any\", available_fns)\n",
        "\n",
        "response = chat.send_message(\"Make this place Orange!\", tool_config=tool_config)\n",
        "print(response.parts[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cGrRy-uJ7-J"
      },
      "source": [
        "### Automatic function calling\n",
        "\n",
        "`tool_config` works when enabling automatic function calling too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx7aIX8OXvi6",
        "outputId": "1322aa95-5d7b-4c48-d628-ba5e13831f9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LIGHTBOT: Lights enabled.\n",
            "LIGHTBOT: Lights set to #FFFF00.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "response:\n",
              "GenerateContentResponse(\n",
              "    done=True,\n",
              "    iterator=None,\n",
              "    result=protos.GenerateContentResponse({\n",
              "      \"candidates\": [\n",
              "        {\n",
              "          \"content\": {\n",
              "            \"parts\": [\n",
              "              {\n",
              "                \"text\": \"Lights are now set to yellow. anything else?\"\n",
              "              }\n",
              "            ],\n",
              "            \"role\": \"model\"\n",
              "          },\n",
              "          \"finish_reason\": \"STOP\",\n",
              "          \"avg_logprobs\": -0.07512627840042115\n",
              "        }\n",
              "      ],\n",
              "      \"usage_metadata\": {\n",
              "        \"prompt_token_count\": 246,\n",
              "        \"candidates_token_count\": 10,\n",
              "        \"total_token_count\": 256\n",
              "      }\n",
              "    }),\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "available_fns = [\"turn_on_light\"]\n",
        "tool_config = tool_config_from_mode(\"any\", available_fns)\n",
        "\n",
        "auto_chat = model.start_chat(enable_automatic_function_calling=True)\n",
        "#auto_chat.send_message(\"It's awful dark in here...\", tool_config=tool_config)\n",
        "auto_chat.send_message(\"Set light color in yellow...\", tool_config=tool_config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def power_disco_ball(power: bool) -> bool:\n",
        "    \"\"\"Powers the spinning disco ball.\"\"\"\n",
        "    print(f\"Disco ball is {'spinning!' if power else 'stopped.'}\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def start_music(energetic: bool, loud: bool, bpm: int) -> str:\n",
        "    \"\"\"Play some music matching the specified parameters.\n",
        "\n",
        "    Args:\n",
        "      energetic: Whether the music is energetic or not.\n",
        "      loud: Whether the music is loud or not.\n",
        "      bpm: The beats per minute of the music.\n",
        "\n",
        "    Returns: The name of the song being played.\n",
        "    \"\"\"\n",
        "    print(f\"Starting music! {energetic=} {loud=}, {bpm=}\")\n",
        "    return \"Never gonna give you up.\"\n",
        "\n",
        "\n",
        "def dim_lights(brightness: float) -> bool:\n",
        "    \"\"\"Dim the lights.\n",
        "\n",
        "    Args:\n",
        "      brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n",
        "    \"\"\"\n",
        "    print(f\"Lights are now set to {brightness:.0%}\")\n",
        "    return True"
      ],
      "metadata": {
        "id": "lKszbil9L_ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model up with tools.\n",
        "house_fns = [power_disco_ball, start_music, dim_lights]\n",
        "\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", tools=house_fns)\n",
        "\n",
        "# Call the API.\n",
        "chat = model.start_chat()\n",
        "response = chat.send_message(\"Turn this place into a party !\")\n",
        "\n",
        "# Print out each of the function calls requested from this single call.\n",
        "for part in response.parts:\n",
        "    if fn := part.function_call:\n",
        "        args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n",
        "        print(f\"{fn.name}({args})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "A1vRFoX8MUcf",
        "outputId": "702108c1-99b7-46ef-f263-fbe12d3ca8af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "power_disco_ball(power=True)\n",
            "start_music(loud=True, energetic=True, bpm=120.0)\n",
            "dim_lights(brightness=0.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate the responses from the specified tools.\n",
        "responses = {\n",
        "    \"power_disco_ball\": True,\n",
        "    \"start_music\": \"Never gonna give you up.\",\n",
        "    \"dim_lights\": True,\n",
        "}\n",
        "\n",
        "# Build the response parts.\n",
        "response_parts = [\n",
        "    genai.protos.Part(function_response=genai.protos.FunctionResponse(name=fn, response={\"result\": val}))\n",
        "    for fn, val in responses.items()\n",
        "]\n",
        "\n",
        "response = chat.send_message(response_parts)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "llxBeFpZNPwf",
        "outputId": "397ff898-9149-43ad-d051-9fcb24e10f2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ok, I've turned on the disco ball, started playing some energetic music (Never gonna give you up), and dimmed the lights to 50% brightness. Let's party!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def multiply(a:float, b:float):\n",
        "    \"\"\"returns a * b.\"\"\"\n",
        "    return a*b\n",
        "\n",
        "model = genai.GenerativeModel(model_name='gemini-1.5-flash',\n",
        "                             tools=[multiply])\n",
        "\n",
        "model._tools.to_proto()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llqE8R3YOIr-",
        "outputId": "0251b450-259b-4237-cba5-88ed4cac6029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[function_declarations {\n",
              "   name: \"multiply\"\n",
              "   description: \"returns a * b.\"\n",
              "   parameters {\n",
              "     type_: OBJECT\n",
              "     properties {\n",
              "       key: \"b\"\n",
              "       value {\n",
              "         type_: NUMBER\n",
              "       }\n",
              "     }\n",
              "     properties {\n",
              "       key: \"a\"\n",
              "       value {\n",
              "         type_: NUMBER\n",
              "       }\n",
              "     }\n",
              "     required: \"a\"\n",
              "     required: \"b\"\n",
              "   }\n",
              " }]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task"
      ],
      "metadata": {
        "id": "zeQq1elTZwDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: make a list of dictionary , key samples is 10 samples of person, which include name, father name , education and address..\n",
        "\n",
        "import random\n",
        "\n",
        "def generate_person_data():\n",
        "  names = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Jona\", \"Alex\", \"Ema\", \"Sophia\", \"William\"]\n",
        "  father_names = [\"Robert\", \"Michael\", \"James\", \"John\", \"David\", \"Richard\", \"Joseph\", \"Thomas\", \"Charles\", \"Christopher\"]\n",
        "  educations = [\"High School\", \"Bachelor's\", \"Master's\", \"PhD\", \"Associate's\"]\n",
        "  addresses = [\"123 Main St\", \"456 Oak Ave\", \"789 Pine Ln\", \"1011 Elm St\", \"1213 Maple Dr\"]\n",
        "\n",
        "  name = random.choice(names)\n",
        "  father_name = random.choice(father_names)\n",
        "  education = random.choice(educations)\n",
        "  address = random.choice(addresses)\n",
        "  return {\n",
        "      \"name\": name,\n",
        "      \"father_name\": father_name,\n",
        "      \"education\": education,\n",
        "      \"address\": address\n",
        "  }\n",
        "\n",
        "\n",
        "samples = []\n",
        "for _ in range(10):\n",
        "  samples.append(generate_person_data())\n",
        "\n",
        "samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKDPrnWOZT0w",
        "outputId": "124f1fa8-e803-462e-840f-1617e0c05677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'Alex',\n",
              "  'father_name': 'Charles',\n",
              "  'education': \"Associate's\",\n",
              "  'address': '789 Pine Ln'},\n",
              " {'name': 'Bob',\n",
              "  'father_name': 'Charles',\n",
              "  'education': \"Associate's\",\n",
              "  'address': '1213 Maple Dr'},\n",
              " {'name': 'Alex',\n",
              "  'father_name': 'Robert',\n",
              "  'education': \"Associate's\",\n",
              "  'address': '123 Main St'},\n",
              " {'name': 'David',\n",
              "  'father_name': 'James',\n",
              "  'education': \"Master's\",\n",
              "  'address': '456 Oak Ave'},\n",
              " {'name': 'Ema',\n",
              "  'father_name': 'Thomas',\n",
              "  'education': 'High School',\n",
              "  'address': '1213 Maple Dr'},\n",
              " {'name': 'David',\n",
              "  'father_name': 'David',\n",
              "  'education': 'PhD',\n",
              "  'address': '789 Pine Ln'},\n",
              " {'name': 'Charlie',\n",
              "  'father_name': 'David',\n",
              "  'education': \"Master's\",\n",
              "  'address': '456 Oak Ave'},\n",
              " {'name': 'David',\n",
              "  'father_name': 'David',\n",
              "  'education': \"Bachelor's\",\n",
              "  'address': '1213 Maple Dr'},\n",
              " {'name': 'Eve',\n",
              "  'father_name': 'Thomas',\n",
              "  'education': 'PhD',\n",
              "  'address': '1213 Maple Dr'},\n",
              " {'name': 'Bob',\n",
              "  'father_name': 'Joseph',\n",
              "  'education': \"Bachelor's\",\n",
              "  'address': '789 Pine Ln'}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Make a function with above list when we ask name as a input, it will give information to that person.\n",
        "\n",
        "def get_person_info(name):\n",
        "    for person in samples:\n",
        "        if person[\"name\"] == name:\n",
        "            return person\n",
        "    return \"No information found for that person.\"\n",
        "\n",
        "# Example usage\n",
        "person_name = input(\"Enter the name of the person: \")\n",
        "info = get_person_info(person_name)\n",
        "info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfbLGb26ZX5Z",
        "outputId": "a5cc6ea0-9061-4679-b79b-6e9eccb5aa8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the name of the person: Bob\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Bob',\n",
              " 'father_name': 'Charles',\n",
              " 'education': \"Associate's\",\n",
              " 'address': '1213 Maple Dr'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task with LLM"
      ],
      "metadata": {
        "id": "g0lJgYU8bLsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: make a list of dictionary , key samples is 10 samples of person, which include name, father name , education and address.. include LLM in it\n",
        "\n",
        "import random\n",
        "\n",
        "def generate_person_data():\n",
        "    names = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Jona\", \"Sophia\", \"William\", \"Olivia\", \"James\"]\n",
        "    father_names = [\"Robert\", \"Michael\", \"William\", \"David\", \"Richard\", \"Joseph\", \"Thomas\", \"Charles\", \"Christopher\", \"Daniel\"]\n",
        "    educations = [\"High School\", \"Bachelor's Degree\", \"Master's Degree\", \"PhD\", \"Associate's Degree\"]\n",
        "    addresses = [\"123 Main St\", \"456 Oak Ave\", \"789 Pine Ln\", \"1011 Elm St\", \"1213 Maple Dr\"]\n",
        "\n",
        "    name = random.choice(names)\n",
        "    father_name = random.choice(father_names)\n",
        "    education = random.choice(educations)\n",
        "    address = random.choice(addresses)\n",
        "\n",
        "    return {\n",
        "        \"name\": name,\n",
        "        \"father_name\": father_name,\n",
        "        \"education\": education,\n",
        "        \"address\": address,\n",
        "    }\n",
        "\n",
        "samples = []\n",
        "for _ in range(10):\n",
        "    samples.append(generate_person_data())\n",
        "\n",
        "samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8Un4uu2QXLY",
        "outputId": "77631918-0429-4363-8211-fd811afb70d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'Alice',\n",
              "  'father_name': 'Robert',\n",
              "  'education': 'PhD',\n",
              "  'address': '789 Pine Ln'},\n",
              " {'name': 'Alice',\n",
              "  'father_name': 'Daniel',\n",
              "  'education': \"Master's Degree\",\n",
              "  'address': '1011 Elm St'},\n",
              " {'name': 'David',\n",
              "  'father_name': 'Daniel',\n",
              "  'education': \"Master's Degree\",\n",
              "  'address': '1011 Elm St'},\n",
              " {'name': 'James',\n",
              "  'father_name': 'David',\n",
              "  'education': 'PhD',\n",
              "  'address': '789 Pine Ln'},\n",
              " {'name': 'Sophia',\n",
              "  'father_name': 'William',\n",
              "  'education': \"Associate's Degree\",\n",
              "  'address': '456 Oak Ave'},\n",
              " {'name': 'David',\n",
              "  'father_name': 'Charles',\n",
              "  'education': 'High School',\n",
              "  'address': '123 Main St'},\n",
              " {'name': 'Bob',\n",
              "  'father_name': 'Christopher',\n",
              "  'education': 'High School',\n",
              "  'address': '1213 Maple Dr'},\n",
              " {'name': 'William',\n",
              "  'father_name': 'Christopher',\n",
              "  'education': 'PhD',\n",
              "  'address': '789 Pine Ln'},\n",
              " {'name': 'Eve',\n",
              "  'father_name': 'Michael',\n",
              "  'education': 'High School',\n",
              "  'address': '1011 Elm St'},\n",
              " {'name': 'William',\n",
              "  'father_name': 'Daniel',\n",
              "  'education': 'High School',\n",
              "  'address': '1213 Maple Dr'}]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Make a function with above list when we ask name as a input, it will give information to that person and recheck by LLM with last cell\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def get_person_info(name, samples):\n",
        "  \"\"\"\n",
        "  Retrieves information about a person from a list of dictionaries.\n",
        "\n",
        "  Args:\n",
        "    name: The name of the person to search for.\n",
        "    samples: A list of dictionaries, where each dictionary represents a person and\n",
        "             contains keys like \"name\", \"father_name\", \"education\", and \"address\".\n",
        "\n",
        "  Returns:\n",
        "    A dictionary containing the person's information if found, otherwise None.\n",
        "  \"\"\"\n",
        "  for person in samples:\n",
        "    if person[\"name\"] == name:\n",
        "      return person\n",
        "  return None\n",
        "\n",
        "def verify_with_llm(person_info):\n",
        "  \"\"\"\n",
        "  Verifies the person's information using a large language model.\n",
        "\n",
        "  Args:\n",
        "    person_info: A dictionary containing the person's information.\n",
        "\n",
        "  Returns:\n",
        "    A string representing the LLM's verification.\n",
        "  \"\"\"\n",
        "\n",
        "  # Construct prompt for LLM based on person's information\n",
        "  prompt = f\"\"\"Verify the following information:\\nName: {person_info['name']}\\nFather's Name: {person_info['father_name']}\\nEducation: {person_info['education']}\\nAddress: {person_info['address']}\n",
        "  \\nIs this information likely to be correct?  Respond with 'Correct' or 'Incorrect' only\"\"\"\n",
        "\n",
        "  # Replace with actual LLM call using the google.generativeai library\n",
        "  # The following is a placeholder; uncomment and modify as needed\n",
        "  # response = genai.generate_text(prompt=prompt, model='models/gemini-pro')\n",
        "  # verification_result = response.result\n",
        "\n",
        "  # Placeholder result (replace with actual LLM response)\n",
        "  verification_result = \"Correct\" # Example: Replace with actual LLM output\n",
        "\n",
        "  return verification_result\n",
        "\n",
        "# Example usage\n",
        "name_to_find = input(\"Enter the name: \")\n",
        "person_info = get_person_info(name_to_find, samples)\n",
        "\n",
        "if person_info:\n",
        "  print(f\"Information for {name_to_find}:\")\n",
        "  print(person_info)\n",
        "\n",
        "  verification = verify_with_llm(person_info)\n",
        "  print(f\"\\nLLM Verification: {verification}\")\n",
        "\n",
        "else:\n",
        "  print(f\"No information found for {name_to_find}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwWTkf5FRi3S",
        "outputId": "0fa82cfe-5f23-4f80-8696-af6e2eb2fdbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the name: Alice\n",
            "Information for Alice:\n",
            "{'name': 'Alice', 'father_name': 'Robert', 'education': 'PhD', 'address': '789 Pine Ln'}\n",
            "\n",
            "LLM Verification: Correct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. LangChain Function/Tool Calling Calculator"
      ],
      "metadata": {
        "id": "AR_qyBezvrkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community langchain_google_genai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kxQt--qQwU_D",
        "outputId": "fa31ea73-90bd-4a21-fe53-6f900dd39ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.14)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.0.8-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.11)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.29)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.10)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (0.8.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.25.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.155.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.25.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.25.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (24.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.13)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.66.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.69.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (3.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading langchain_community-0.3.14-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.0.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.25.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: filetype, python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_google_genai, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 filetype-1.2.0 httpx-sse-0.4.0 langchain_community-0.3.14 langchain_google_genai-2.0.8 marshmallow-3.25.1 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n"
      ],
      "metadata": {
        "id": "mcVbp0XcwsCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm=ChatGoogleGenerativeAI(model='gemini-1.5-flash',verbose=True)\n",
        "\n",
        "llm.invoke(\"Hi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfF5E8lxwvg8",
        "outputId": "84dca718-5def-4308-c32f-bab772338fa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-f1988a40-bb4e-425b-9b94-5608fc2fb0c1-0', usage_metadata={'input_tokens': 2, 'output_tokens': 11, 'total_tokens': 13, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "llm_with_tools = llm.bind_tools([multiply])"
      ],
      "metadata": {
        "id": "f2sb1Ff8wx8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools_list=[multiply]\n",
        "model_with_tools = llm.bind_tools(tools_list)"
      ],
      "metadata": {
        "id": "dBM3IkO0w0Ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm_with_tools.invoke(\"Hey how are you!\")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EfW-HPEw4bQ",
        "outputId": "8623b579-0f27-46ed-ba04-2d8822db0bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm doing well, thank you for asking! How are you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-b4b22658-c325-4e72-8476-f0518059182a-0', usage_metadata={'input_tokens': 68, 'output_tokens': 17, 'total_tokens': 85, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm_with_tools.invoke(\"What is 2 multiplied by 3?\")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6K8_OJVw4_t",
        "outputId": "f8a1dfbe-d39f-4408-f272-509c2f978022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"a\": 2.0, \"b\": 3.0}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-98199979-e760-4288-8dd5-608f0c63b6fd-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2.0, 'b': 3.0}, 'id': '1c4dd385-50a1-4fbd-8c1f-268ccb816911', 'type': 'tool_call'}], usage_metadata={'input_tokens': 72, 'output_tokens': 3, 'total_tokens': 75, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import Tool\n",
        "\n",
        "# Example tool for addition\n",
        "def add_numbers(a, b):\n",
        "    return a + b\n",
        "\n",
        "math_tool = Tool(\n",
        "    name=\"AdditionTool\",\n",
        "    func=lambda x: f\"The result is: {add_numbers(*map(int, x.split()))}\",\n",
        "    description=\"Adds two numbers. Input should be two integers separated by a space.\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "uRVZeUMUw60c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, AgentType\n",
        "\n",
        "# List of tools\n",
        "tools = [math_tool]\n",
        "\n",
        "# Initialize the agent\n",
        "agent = initialize_agent(\n",
        "    tools=tools,                        # Provide the tools\n",
        "    llm=llm,                            # LLM for fallback\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # Agent type\n",
        "    verbose=True                        # Enable debugging output\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14MUqJwKw9AS",
        "outputId": "9b8f2dfd-8644-4a76-c7e4-6f263ed29757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-f11f73b77070>:7: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = initialize_agent(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the agent\n",
        "response1 = agent.run(\"What is 10 + 20?\")  # Uses the tool\n",
        "print(response1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgMpZJTGw_OD",
        "outputId": "3ab07bfd-b050-459d-9d29-92f7e06b7d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to add 10 and 20. I can use the AdditionTool for this.\n",
            "Action: AdditionTool\n",
            "Action Input: 10 20\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mThe result is: 30\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
            "Final Answer: 30\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "30\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rZfOTN-qvfbA",
        "lXOGXSyGyUrD",
        "iJqil-VL8ug-",
        "ofMEuh_MFdMf",
        "uux063sjHZ_Z",
        "oHhaO-P9CBPb",
        "8cGrRy-uJ7-J",
        "zeQq1elTZwDe",
        "AR_qyBezvrkW"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}