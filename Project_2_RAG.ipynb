{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XLGjOdQzHZrF",
        "NfUj3_ruMy9F",
        "wEW3HyTMM5dJ",
        "vzw3ZWslNoY3",
        "d5Sx0NM4Nyv8",
        "rWsEiUWCOe8l",
        "jGxHy59HOoYw",
        "SwqZETVQP9gP",
        "ttExoGOpjwgQ",
        "87iczWdxkBMu"
      ],
      "authorship_tag": "ABX9TyPa7Qy7mbWXWffb6yfqa9iS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tasneem-Ibrahim/Python/blob/main/Project_2_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project 2 - RAG\n",
        "LangChain RAG with Google Gemini Flash and Pinecone"
      ],
      "metadata": {
        "id": "PsNgbyOGHUn3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Set Up Environment Variables"
      ],
      "metadata": {
        "id": "XLGjOdQzHZrF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5Tq9nArFHCl0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b76f3a3c-7637-4614-ab78-d0deb177f5c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/454.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain-pinecone langchain-google-genai google-generativeai openai tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Import Libraries and Initialize Pinecone"
      ],
      "metadata": {
        "id": "NfUj3_ruMy9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#import getpass\n",
        "#import os\n",
        "#import time\n",
        "\n",
        "from google.colab import userdata\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pinecone_api_key = userdata.get('PINECONE_API_KEY')\n",
        "pc = Pinecone(api_key=pinecone_api_key)"
      ],
      "metadata": {
        "id": "WfFarh12Hnmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Create or Connect to Pinecone Index"
      ],
      "metadata": {
        "id": "wEW3HyTMM5dJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "index_name = \"online-rag-project\"  # change if desired\n",
        "\n",
        "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "\n",
        "if index_name not in existing_indexes:\n",
        "    pc.create_index(\n",
        "      name=index_name,\n",
        "      dimension=768,\n",
        "      metric=\"cosine\",\n",
        "      spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
        "       time.sleep(1)\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "vtYq0d8QMBiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Initialize Google Generative AI Embeddings"
      ],
      "metadata": {
        "id": "oM8JtZciNE_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import os\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
      ],
      "metadata": {
        "id": "VNvINffAedQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Generate Query Embedding"
      ],
      "metadata": {
        "id": "30zTHkByNhcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = embeddings.embed_query(\"Building a system leveraging RAG for better responses. \")\n",
        "vector[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJSDqnLeBbTE",
        "outputId": "be37a87f-af15-43a2-b190-1defb4e99f05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.03307317569851875,\n",
              " -0.05718652531504631,\n",
              " -0.013254886493086815,\n",
              " 0.029134679585695267,\n",
              " 0.002929301466792822]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Create Pinecone Vector Store"
      ],
      "metadata": {
        "id": "vzw3ZWslNoY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "id": "QG4xE3VzNGDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Prepare Documents for Vector Store"
      ],
      "metadata": {
        "id": "d5Sx0NM4Nyv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Save\n",
        "\n",
        "from uuid import uuid4\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_2 = Document(\n",
        "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_3 = Document(\n",
        "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_4 = Document(\n",
        "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_5 = Document(\n",
        "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_6 = Document(\n",
        "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_7 = Document(\n",
        "    page_content=\"The top 10 soccer players in the world right now.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_8 = Document(\n",
        "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_9 = Document(\n",
        "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_10 = Document(\n",
        "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    document_1,\n",
        "    document_2,\n",
        "    document_3,\n",
        "    document_4,\n",
        "    document_5,\n",
        "    document_6,\n",
        "    document_7,\n",
        "    document_8,\n",
        "    document_9,\n",
        "    document_10,\n",
        "]\n"
      ],
      "metadata": {
        "id": "kP8VhXHNNLbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PC8arEOFP7yS",
        "outputId": "56ca4b56-aa92-4fc9-896c-4d9473d72af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 8: Generate UUIDs and Add Documents"
      ],
      "metadata": {
        "id": "rWsEiUWCOe8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "uuid4()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNWzh3RMQkhq",
        "outputId": "fd061f1c-4b8b-4ddf-8b38-682202641b3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UUID('535d3031-5dba-46c5-86a0-b1643b9fbe17')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "vector_store.add_documents(documents=documents, ids=uuids)\n",
        "\n",
        "# Delete items from vector store:\n",
        "# vector_store.delete(ids=[uuids[-1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTiq5kdePqox",
        "outputId": "d62a17e8-5bc0-42f6-9134-83d6263de166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['9f58d6ea-88fb-4c50-9594-4ac212fc8346',\n",
              " '47d68710-f5af-41b2-9364-ad9667f025d1',\n",
              " '9b0ab6bb-9e16-4f72-bee5-a1ed53bcbc6f',\n",
              " '0b22f3c3-1882-47da-851b-1f6d81c99276',\n",
              " '075326bc-9b7e-4723-bd33-8991dff11b64',\n",
              " '182cb806-3eac-4e7a-9b89-2bf075a10e71',\n",
              " '40320eca-1beb-4422-af82-f1af7bff1957',\n",
              " 'b1aeca40-e34a-4324-a389-21c307225788',\n",
              " '4aaf8116-b47d-4958-acaf-5dd67963f726',\n",
              " '977cb7e0-48e9-44da-853d-8c58cba99546']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 9: Perform Similarity Search"
      ],
      "metadata": {
        "id": "jGxHy59HOoYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Reterive\n",
        "results = vector_store.similarity_search(\n",
        "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
        "    k=5,\n",
        "    filter={\"source\": \"tweet\"},\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "id": "g6IcF3M2SD4j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f562ba1-10b1-478a-cbdf-8ff57c85516f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n",
            "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n",
            "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n",
            "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n",
            "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search_with_score(\n",
        "    \"Will it be hot tomorrow?\", k=1, filter={\"source\": \"news\"}\n",
        ")\n",
        "for res, score in results:\n",
        "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "id": "A72SS61-PqCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb84ab03-1a02-431e-e499-74c1c17b9fd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* [SIM=0.668031] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query by turning into retriever"
      ],
      "metadata": {
        "id": "DUd7PqXbO3SL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\",\n",
        "    search_kwargs={\"k\": 1, \"score_threshold\": 0.5},\n",
        ")\n",
        "retriever.invoke(\"Stealing from the bank is a crime\", filter={\"source\": \"news\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx4k1a0jPBJ9",
        "outputId": "0525b37b-a924-4a88-f8a3-c50d97c2670b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='20eedb21-2968-483d-9520-43eaf3350854', metadata={'source': 'news'}, page_content='Robbers broke into the city bank and stole $1 million in cash.')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 10: Configuring Google's Gemini Model for Conversational AI"
      ],
      "metadata": {
        "id": "SwqZETVQP9gP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")"
      ],
      "metadata": {
        "id": "nTiphB1fP7UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 11: Function to Generate User-Specific Answers Using Vector Search and LLM"
      ],
      "metadata": {
        "id": "ttExoGOpjwgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_to_user(query: str):\n",
        "\n",
        "    vector_results = vector_store.similarity_search(query, k=2)\n",
        "    print(len(vector_results ))\n",
        "\n",
        "    # ToDo pass to vector model result + User query\n",
        "    # Final answer by invoking the LLM\n",
        "    final_answer = llm.invoke(f\"ANSWER THIS USER QUERY: {query} Here are some references to answer:\\n{vector_results}\")\n",
        "\n",
        "    return final_answer\n"
      ],
      "metadata": {
        "id": "C2kn5N_QP7vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Executing User Query and Retrieving Final Answer from LLM"
      ],
      "metadata": {
        "id": "87iczWdxkBMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = answer_to_user(\"LangGraph is the best framework for building stateful, agentic applications!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQvUEZtmP8M3",
        "outputId": "9a349229-ccc7-4a9f-8bae-ceb4403d2382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFaCirE2P8bO",
        "outputId": "a09906cd-65d5-46c3-8e9e-dc57a3e8fdbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The provided text only contains two tweets stating that LangGraph is the best framework for building stateful, agentic applications.  There is no further information or evidence to support this claim.  Therefore, the assertion that LangGraph is the *best* framework is unsubstantiated.  While the tweets express a strong opinion,  more information and comparative analysis with other frameworks would be needed to validate this statement.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}