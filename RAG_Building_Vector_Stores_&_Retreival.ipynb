{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tasneem-Ibrahim/Python/blob/main/RAG_Building_Vector_Stores_%26_Retreival.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evjGLA8Jx5k8"
      },
      "outputs": [],
      "source": [
        "# Pakistan zinda bad, we love our country.\n",
        "# 0         1     2    3  4.   5.   6\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "jgK8e2vq6gvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88499ebc-dd9f-4ea5-8d1f-fb8609c001dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/175.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m163.8/175.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))"
      ],
      "metadata": {
        "id": "U5a_LR0V7QEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(genai.list_models())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "3Q5zbNgD72OA",
        "outputId": "b37744ab-8984-47c4-8117-04eac1bdf36a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(name='models/chat-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 Chat (Legacy)',\n",
              "       description='A legacy text-only model optimized for chat conversations',\n",
              "       input_token_limit=4096,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
              "       temperature=0.25,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/embedding-gecko-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding Gecko',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=1024,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The original Gemini 1.0 Pro model. This model will be discontinued on '\n",
              "                    'February 15th, 2025. Move to a newer Gemini version.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The original Gemini 1.0 Pro model version that supports tuning. Gemini 1.0 '\n",
              "                    'Pro will be discontinued on February 15th, 2025. Move to a newer Gemini '\n",
              "                    'version.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
              "                    'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
              "                    'Move to a newer Gemini version.'),\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
              "                    'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
              "                    'Move to a newer Gemini version.'),\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-1.5-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro Latest',\n",
              "       description=('Alias that points to the most recent production (non-experimental) release '\n",
              "                    'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
              "                    'million tokens.'),\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro 001',\n",
              "       description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
              "                    'supports up to 2 million tokens, released in May of 2024.'),\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-002',\n",
              "       base_model_id='',\n",
              "       version='002',\n",
              "       display_name='Gemini 1.5 Pro 002',\n",
              "       description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
              "                    'supports up to 2 million tokens, released in September of 2024.'),\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro',\n",
              "       description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
              "                    'supports up to 2 million tokens, released in May of 2024.'),\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-pro-exp-0801',\n",
              "       base_model_id='',\n",
              "       version='exp-0801',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-1206',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash Latest',\n",
              "       description=('Alias that points to the most recent production (non-experimental) release '\n",
              "                    'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
              "                    'across diverse tasks.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001',\n",
              "       description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
              "                    'for scaling across diverse tasks, released in May of 2024.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001-tuning',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001 Tuning',\n",
              "       description=('Version of Gemini 1.5 Flash that supports tuning, our fast and versatile '\n",
              "                    'multimodal model for scaling across diverse tasks, released in May of 2024.'),\n",
              "       input_token_limit=16384,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash',\n",
              "       description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
              "                    'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-1206',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-002',\n",
              "       base_model_id='',\n",
              "       version='002',\n",
              "       display_name='Gemini 1.5 Flash 002',\n",
              "       description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
              "                    'for scaling across diverse tasks, released in September of 2024.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash-8B',\n",
              "       description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
              "                    'Flash model, released in October of 2024.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash-8B 001',\n",
              "       description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
              "                    'Flash model, released in October of 2024.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash-8B Latest',\n",
              "       description=('Alias that points to the most recent production (non-experimental) release '\n",
              "                    'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
              "                    'released in October of 2024.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
              "       description=('Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our '\n",
              "                    'smallest and most cost effective Flash model. Replaced by '\n",
              "                    'Gemini-1.5-flash-8b-001 (stable).'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b-exp-0924',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 8B Experimental 0924',\n",
              "       description=('Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our '\n",
              "                    'smallest and most cost effective Flash model. Replaced by '\n",
              "                    'Gemini-1.5-flash-8b-001 (stable).'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-2.0-flash-exp',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash Experimental',\n",
              "       description='Gemini 2.0 Flash Experimental',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-exp-1206',\n",
              "       base_model_id='',\n",
              "       version='exp_1206',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-exp-1121',\n",
              "       base_model_id='',\n",
              "       version='exp-1206',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-exp-1114',\n",
              "       base_model_id='',\n",
              "       version='exp-1206',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.0-flash-thinking-exp-01-21',\n",
              "       base_model_id='',\n",
              "       version='2.0-exp-01-21',\n",
              "       display_name='Gemini 2.0 Flash Thinking Experimental 01-21',\n",
              "       description='Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.7,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.0-flash-thinking-exp',\n",
              "       base_model_id='',\n",
              "       version='2.0-exp-01-21',\n",
              "       display_name='Gemini 2.0 Flash Thinking Experimental 01-21',\n",
              "       description='Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.7,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.0-flash-thinking-exp-1219',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash Thinking Experimental',\n",
              "       description='Gemini 2.0 Flash Thinking Experimental',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.7,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/learnlm-1.5-pro-experimental',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='LearnLM 1.5 Pro Experimental',\n",
              "       description=('Alias that points to the most recent stable version of Gemini 1.5 Pro, our '\n",
              "                    'mid-size multimodal model that supports up to 2 million tokens.'),\n",
              "       input_token_limit=32767,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/embedding-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding 001',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/text-embedding-004',\n",
              "       base_model_id='',\n",
              "       version='004',\n",
              "       display_name='Text Embedding 004',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/aqa',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Model that performs Attributed Question Answering.',\n",
              "       description=('Model trained to return answers to questions that are grounded in provided '\n",
              "                    'sources, along with estimating answerable probability.'),\n",
              "       input_token_limit=7168,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateAnswer'],\n",
              "       temperature=0.2,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=40)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from typing import Dict\n",
        "\n",
        "result : Dict = genai.embed_content(\n",
        "    model=\"models/text-embedding-004\",\n",
        "    content=\"What is the meaning of life?\",\n",
        "    task_type=\"retrieval_document\",\n",
        "    title=\"Embedding of single string\",\n",
        ")\n",
        "\n",
        "# # 1 input > 1 vector output\n",
        "# print(str(result[\"embedding\"])[:50], \"... TRIMMED]\")\n",
        "\n",
        "result['embedding']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wslvg_cS6hQG",
        "outputId": "7a47c6a4-0d63-4ea0-c7ff-7e337cdd8b10",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.02854543,\n",
              " 0.044588115,\n",
              " -0.034197364,\n",
              " -0.0042663575,\n",
              " -0.04079577,\n",
              " 0.012999958,\n",
              " 0.018053582,\n",
              " 0.06015144,\n",
              " -0.0028713925,\n",
              " 0.009951648,\n",
              " 0.024832657,\n",
              " -0.01683923,\n",
              " 0.09940116,\n",
              " -0.031990346,\n",
              " 0.018328529,\n",
              " -0.109134205,\n",
              " 0.001190296,\n",
              " 0.0014311911,\n",
              " -0.083155245,\n",
              " -0.010203233,\n",
              " 0.019211812,\n",
              " 0.0010217889,\n",
              " 0.053874534,\n",
              " -0.0150861535,\n",
              " -0.003189089,\n",
              " 0.019626662,\n",
              " -0.0074312133,\n",
              " -0.036586244,\n",
              " -0.008509182,\n",
              " -0.017352631,\n",
              " 0.058202818,\n",
              " 0.05446324,\n",
              " 0.01571296,\n",
              " -0.021822602,\n",
              " 0.048009068,\n",
              " 0.022641798,\n",
              " -0.0069730366,\n",
              " 0.054272633,\n",
              " 0.025922865,\n",
              " -0.027334303,\n",
              " -0.07256842,\n",
              " 0.028509492,\n",
              " -0.03564165,\n",
              " 0.060492564,\n",
              " -0.022731686,\n",
              " -0.030770157,\n",
              " -0.006176277,\n",
              " -0.021891864,\n",
              " -0.019659325,\n",
              " 0.0643669,\n",
              " 0.03154234,\n",
              " 0.017379418,\n",
              " -0.03679774,\n",
              " 0.016511764,\n",
              " -0.02536976,\n",
              " -0.022270117,\n",
              " -0.012396498,\n",
              " -0.032805424,\n",
              " 0.054154944,\n",
              " -0.04823156,\n",
              " -0.021759441,\n",
              " -0.03370158,\n",
              " -0.025460402,\n",
              " -0.017531719,\n",
              " -0.052902102,\n",
              " 0.04005264,\n",
              " -0.022417234,\n",
              " 0.023286799,\n",
              " -0.081740536,\n",
              " 0.057951722,\n",
              " -0.009130241,\n",
              " 0.040680293,\n",
              " -0.026519705,\n",
              " 0.03940553,\n",
              " -0.009711097,\n",
              " 0.034678303,\n",
              " 0.022007594,\n",
              " 0.013833644,\n",
              " -0.037151057,\n",
              " 0.03425778,\n",
              " -0.037425302,\n",
              " 0.0133839715,\n",
              " 0.062417556,\n",
              " 0.05847619,\n",
              " 0.010972301,\n",
              " 0.0069226264,\n",
              " -0.028345693,\n",
              " -0.025934424,\n",
              " -0.08300387,\n",
              " -0.018701842,\n",
              " 0.051876634,\n",
              " 0.028144632,\n",
              " 0.0050556627,\n",
              " 0.032798667,\n",
              " 0.065246835,\n",
              " -0.030897865,\n",
              " -0.05739043,\n",
              " -0.075857975,\n",
              " 0.013478357,\n",
              " 0.06728078,\n",
              " 0.058566026,\n",
              " 0.004352499,\n",
              " 0.004930005,\n",
              " -0.04687863,\n",
              " 0.047707375,\n",
              " 0.09346361,\n",
              " -0.033218864,\n",
              " -0.031682696,\n",
              " -0.035492342,\n",
              " -0.01704582,\n",
              " 0.0070146834,\n",
              " -0.070768684,\n",
              " 0.033927422,\n",
              " -0.05199852,\n",
              " 0.023873666,\n",
              " -0.07814866,\n",
              " -0.0045958003,\n",
              " 0.008596137,\n",
              " -0.014557764,\n",
              " -0.005185891,\n",
              " 0.019220637,\n",
              " 0.06736503,\n",
              " -0.0036469845,\n",
              " 0.029749395,\n",
              " 0.06519027,\n",
              " -0.0017471175,\n",
              " 0.0032194257,\n",
              " -0.058492143,\n",
              " -0.04441574,\n",
              " -0.02082184,\n",
              " 0.09642446,\n",
              " -0.1019518,\n",
              " -0.014023612,\n",
              " 0.0124329,\n",
              " -0.03430263,\n",
              " 0.017259782,\n",
              " 0.09509829,\n",
              " -0.0046976367,\n",
              " 0.014962477,\n",
              " -0.014655782,\n",
              " 0.013355405,\n",
              " -0.025665203,\n",
              " -0.044942174,\n",
              " 0.025182499,\n",
              " 0.017465375,\n",
              " -0.03649158,\n",
              " 0.041676275,\n",
              " 0.0115632955,\n",
              " -0.059281666,\n",
              " -0.0076066344,\n",
              " -0.020777298,\n",
              " -0.02422031,\n",
              " 0.062161032,\n",
              " 0.004218794,\n",
              " -0.008247845,\n",
              " 0.0025803575,\n",
              " 0.063738205,\n",
              " -0.0257109,\n",
              " 0.095686235,\n",
              " -0.027317889,\n",
              " -0.0018857537,\n",
              " -0.078989305,\n",
              " -0.03540763,\n",
              " 0.0067127054,\n",
              " -0.048317876,\n",
              " -0.030285196,\n",
              " 0.023211101,\n",
              " -0.0491988,\n",
              " 0.010719925,\n",
              " 0.02444096,\n",
              " -0.0052104676,\n",
              " 0.008697184,\n",
              " -0.07112967,\n",
              " -0.033053268,\n",
              " -0.024818258,\n",
              " 0.0080466885,\n",
              " -0.022770239,\n",
              " -0.0047805742,\n",
              " -0.010209843,\n",
              " 0.03673981,\n",
              " 0.09795194,\n",
              " 0.03386203,\n",
              " -0.010665188,\n",
              " -0.07381909,\n",
              " 0.00068866805,\n",
              " -0.025717223,\n",
              " -0.0027137904,\n",
              " 0.04281671,\n",
              " 0.034393556,\n",
              " 0.059981614,\n",
              " -0.056784473,\n",
              " 0.030799013,\n",
              " 0.0035953694,\n",
              " 0.034763683,\n",
              " 0.014165773,\n",
              " -0.04174826,\n",
              " 0.061088957,\n",
              " -0.012045537,\n",
              " -0.051974565,\n",
              " -0.0055955644,\n",
              " 0.007566413,\n",
              " -0.03619617,\n",
              " -0.0193453,\n",
              " -0.038998786,\n",
              " 0.016063262,\n",
              " -0.0075189867,\n",
              " -0.045611285,\n",
              " -0.04747799,\n",
              " 0.047357876,\n",
              " 0.0055609345,\n",
              " -0.029912258,\n",
              " -0.01027596,\n",
              " -0.009503956,\n",
              " -0.022230182,\n",
              " 0.047264725,\n",
              " 0.033944495,\n",
              " 0.0636137,\n",
              " -0.0061410535,\n",
              " 0.10799254,\n",
              " 0.023873555,\n",
              " -0.024063213,\n",
              " -0.01934859,\n",
              " -0.019175187,\n",
              " -0.02375712,\n",
              " 0.010766843,\n",
              " 0.010511031,\n",
              " -0.020831015,\n",
              " -0.0415524,\n",
              " -0.021580799,\n",
              " -0.03839475,\n",
              " 0.015841262,\n",
              " 0.0064048977,\n",
              " 0.00884391,\n",
              " -0.012423147,\n",
              " -0.0060453643,\n",
              " 0.02140582,\n",
              " -0.008806144,\n",
              " 0.037548866,\n",
              " -0.031451035,\n",
              " -0.023714807,\n",
              " 0.009285378,\n",
              " -0.00047728888,\n",
              " -0.018435625,\n",
              " 0.010426646,\n",
              " 0.07636751,\n",
              " 0.07757101,\n",
              " 0.03991539,\n",
              " 0.06917671,\n",
              " 0.0016628958,\n",
              " -0.091617554,\n",
              " -0.026956096,\n",
              " -0.013664855,\n",
              " -0.054339245,\n",
              " -0.08310413,\n",
              " -0.025953747,\n",
              " -0.033568814,\n",
              " 0.034448486,\n",
              " 0.0032975704,\n",
              " 0.015053127,\n",
              " -0.005894113,\n",
              " 0.029622843,\n",
              " -0.08048511,\n",
              " -0.013663298,\n",
              " -0.029040001,\n",
              " -0.043319844,\n",
              " -0.058795128,\n",
              " -0.030551378,\n",
              " -0.03321159,\n",
              " 0.03361637,\n",
              " -0.0583398,\n",
              " 0.048462477,\n",
              " -0.019440303,\n",
              " -0.002224581,\n",
              " -0.013960494,\n",
              " 0.043021582,\n",
              " 0.028916152,\n",
              " 0.013766024,\n",
              " 0.029295404,\n",
              " 0.028375948,\n",
              " -0.012632167,\n",
              " 0.009296993,\n",
              " -0.029445387,\n",
              " -0.00021846336,\n",
              " -0.0015232554,\n",
              " 0.013555971,\n",
              " -0.06027861,\n",
              " 0.0022099994,\n",
              " 0.0020799884,\n",
              " -0.012007119,\n",
              " 0.008591618,\n",
              " 0.048698094,\n",
              " 0.047572628,\n",
              " 0.017961571,\n",
              " -0.04480692,\n",
              " 0.01596784,\n",
              " 0.026624791,\n",
              " 0.04592755,\n",
              " 0.023339162,\n",
              " 0.012650808,\n",
              " 0.014623401,\n",
              " 0.059190698,\n",
              " 0.053951625,\n",
              " -0.012018796,\n",
              " 0.025127882,\n",
              " 0.013777736,\n",
              " 0.008772696,\n",
              " 0.06242213,\n",
              " -0.02985679,\n",
              " 0.015534353,\n",
              " 0.008786879,\n",
              " 0.0021978319,\n",
              " 0.011586468,\n",
              " -0.02561069,\n",
              " 0.031158268,\n",
              " -0.07818875,\n",
              " -0.025129631,\n",
              " -0.15375015,\n",
              " -0.028578915,\n",
              " -0.019318363,\n",
              " -0.020512082,\n",
              " -0.006895941,\n",
              " 0.025423868,\n",
              " 0.016258566,\n",
              " -0.013225587,\n",
              " 0.07140959,\n",
              " -0.03596512,\n",
              " -0.027768183,\n",
              " 0.018441642,\n",
              " 0.0016754153,\n",
              " -0.009046769,\n",
              " 0.0068947347,\n",
              " 0.010378478,\n",
              " -0.004311906,\n",
              " -0.036049224,\n",
              " 0.0429449,\n",
              " -0.00087607105,\n",
              " -0.021248715,\n",
              " 0.039782412,\n",
              " 0.03250818,\n",
              " 0.050120413,\n",
              " -0.011048507,\n",
              " 0.0046043964,\n",
              " 0.012511691,\n",
              " 0.024216538,\n",
              " 0.01150548,\n",
              " -0.02745774,\n",
              " -0.0051559354,\n",
              " -0.007308025,\n",
              " 0.019659724,\n",
              " -0.022055222,\n",
              " 0.00027937818,\n",
              " 0.044644978,\n",
              " 0.013013166,\n",
              " 0.005594769,\n",
              " -0.009612895,\n",
              " -0.025433125,\n",
              " 0.07727911,\n",
              " 0.031199932,\n",
              " 0.038574066,\n",
              " 0.007593594,\n",
              " -0.018812446,\n",
              " -0.012732279,\n",
              " -0.0003708816,\n",
              " -0.017922256,\n",
              " -0.012979617,\n",
              " -0.048761148,\n",
              " 0.013668185,\n",
              " 0.039892722,\n",
              " 0.01600722,\n",
              " -0.015836455,\n",
              " 0.05010714,\n",
              " -0.023271231,\n",
              " -0.0015552061,\n",
              " 0.008744261,\n",
              " -0.0021387269,\n",
              " -0.0138165355,\n",
              " -0.011505079,\n",
              " 0.0383287,\n",
              " 0.032180313,\n",
              " -0.06972529,\n",
              " -0.04253552,\n",
              " -0.035327293,\n",
              " -0.03087898,\n",
              " -0.008225923,\n",
              " -0.057406187,\n",
              " 0.06683099,\n",
              " -0.021594517,\n",
              " -0.005994046,\n",
              " 0.022828693,\n",
              " 0.022839233,\n",
              " -0.03893198,\n",
              " 0.05164902,\n",
              " 0.057929542,\n",
              " 0.021702295,\n",
              " 0.006756328,\n",
              " 0.0020226631,\n",
              " -0.04540625,\n",
              " 0.0411206,\n",
              " -0.0064312797,\n",
              " 0.046170663,\n",
              " -0.02217186,\n",
              " -0.04092706,\n",
              " 0.09603613,\n",
              " 0.010617954,\n",
              " -0.019166118,\n",
              " 0.004992475,\n",
              " 0.08547908,\n",
              " 0.0042816126,\n",
              " -0.011178713,\n",
              " -0.036315963,\n",
              " -0.0397458,\n",
              " 0.0035810445,\n",
              " -0.0073561026,\n",
              " -0.0011699863,\n",
              " -0.058471896,\n",
              " -0.01679719,\n",
              " -0.022455424,\n",
              " 0.004091696,\n",
              " -0.001995662,\n",
              " 0.03065391,\n",
              " 0.00027773026,\n",
              " -0.013209596,\n",
              " 0.0062156287,\n",
              " -0.0019772635,\n",
              " 0.026441118,\n",
              " -0.08084242,\n",
              " 0.00813773,\n",
              " 0.011137467,\n",
              " 0.025003403,\n",
              " 0.039739534,\n",
              " 0.04352838,\n",
              " -0.010362335,\n",
              " -0.005326726,\n",
              " 0.026611479,\n",
              " 0.03678279,\n",
              " 0.020289298,\n",
              " -0.00024059122,\n",
              " 0.028297735,\n",
              " -0.058491826,\n",
              " -0.019358084,\n",
              " -0.009499252,\n",
              " -0.013873281,\n",
              " 0.0032195828,\n",
              " 0.04456959,\n",
              " -0.0029241447,\n",
              " 0.008418838,\n",
              " 0.03416341,\n",
              " 0.012652945,\n",
              " -0.030445265,\n",
              " 0.015912598,\n",
              " -0.015061086,\n",
              " -0.004397588,\n",
              " -0.076873265,\n",
              " -0.022693606,\n",
              " -0.017148094,\n",
              " 0.008034367,\n",
              " -0.021009823,\n",
              " -0.013641983,\n",
              " -0.030586809,\n",
              " 0.063050985,\n",
              " -0.0058573247,\n",
              " -0.03037537,\n",
              " 0.06859101,\n",
              " 0.007416954,\n",
              " 0.013147328,\n",
              " -0.03752198,\n",
              " -0.030428246,\n",
              " 0.053029884,\n",
              " -0.022908261,\n",
              " 0.05607778,\n",
              " 0.05492161,\n",
              " 0.013270513,\n",
              " 0.0070847725,\n",
              " 0.029913813,\n",
              " -0.010489726,\n",
              " 0.007844949,\n",
              " 0.07760543,\n",
              " -0.03246045,\n",
              " -0.013374177,\n",
              " -0.027256502,\n",
              " -0.004605633,\n",
              " 0.030556176,\n",
              " -0.025936672,\n",
              " 0.04827568,\n",
              " 0.044143107,\n",
              " -0.01873767,\n",
              " -0.031347197,\n",
              " -0.026854562,\n",
              " 0.031382143,\n",
              " 0.013992115,\n",
              " 0.021271078,\n",
              " 0.027894089,\n",
              " -0.024505738,\n",
              " -0.07478747,\n",
              " 0.013540895,\n",
              " -0.017613562,\n",
              " 0.01590729,\n",
              " 0.017141188,\n",
              " 0.04708115,\n",
              " 0.019568153,\n",
              " 0.091656715,\n",
              " -0.004865123,\n",
              " -0.019308813,\n",
              " -0.0053966944,\n",
              " -0.0278622,\n",
              " 0.012864926,\n",
              " -0.061174583,\n",
              " -0.041453917,\n",
              " 0.075621046,\n",
              " -0.0070187845,\n",
              " 0.01906601,\n",
              " 0.0054263994,\n",
              " 0.012566397,\n",
              " -0.019087587,\n",
              " -0.043440256,\n",
              " 0.041887432,\n",
              " -0.014445988,\n",
              " 0.04691199,\n",
              " -0.0016946428,\n",
              " 0.071609624,\n",
              " -0.024095738,\n",
              " -0.029031072,\n",
              " 0.0023203683,\n",
              " 0.014458297,\n",
              " 0.010492939,\n",
              " -0.005559697,\n",
              " 0.025936546,\n",
              " 0.009190466,\n",
              " -0.0027095198,\n",
              " -0.013050847,\n",
              " -0.020220019,\n",
              " 0.056056578,\n",
              " 0.03650916,\n",
              " 0.019963812,\n",
              " -0.013089996,\n",
              " 0.05316529,\n",
              " 0.02269551,\n",
              " 0.0141424555,\n",
              " 0.022176167,\n",
              " 0.020095332,\n",
              " 0.011605739,\n",
              " 0.0020735825,\n",
              " 0.003068887,\n",
              " 0.009191726,\n",
              " 0.025579473,\n",
              " -0.008541693,\n",
              " -0.03808776,\n",
              " 0.04622485,\n",
              " -0.029493613,\n",
              " 0.07711512,\n",
              " 0.014877751,\n",
              " -0.029058069,\n",
              " 0.025354061,\n",
              " 0.03530013,\n",
              " 0.0014759303,\n",
              " -0.021830602,\n",
              " 0.02163411,\n",
              " 0.025770994,\n",
              " -0.06815127,\n",
              " 0.021125901,\n",
              " 0.0019301678,\n",
              " 0.002210321,\n",
              " -0.00305364,\n",
              " -0.021801703,\n",
              " -0.051318586,\n",
              " -0.033611473,\n",
              " 0.012315321,\n",
              " 0.0072091133,\n",
              " 0.0014005301,\n",
              " -0.023998646,\n",
              " 0.0026291292,\n",
              " -0.015098267,\n",
              " -0.011970929,\n",
              " -0.030961366,\n",
              " 0.021716155,\n",
              " 0.019829318,\n",
              " -0.0408635,\n",
              " -0.0088035865,\n",
              " 0.02354545,\n",
              " -0.035932545,\n",
              " 0.061214827,\n",
              " 0.005042119,\n",
              " 0.053205136,\n",
              " 0.012700384,\n",
              " -0.0013505232,\n",
              " -0.02219724,\n",
              " 0.017605027,\n",
              " 0.0109856445,\n",
              " -0.0046386044,\n",
              " -0.007891236,\n",
              " -0.025283262,\n",
              " 0.05328586,\n",
              " 0.009774557,\n",
              " -0.036375165,\n",
              " -0.026973603,\n",
              " -0.024808131,\n",
              " -0.0313296,\n",
              " -0.012596162,\n",
              " 0.02893709,\n",
              " 0.007063438,\n",
              " 0.012943186,\n",
              " -0.014554011,\n",
              " 0.022427145,\n",
              " 0.009753417,\n",
              " -0.030994788,\n",
              " -0.095215425,\n",
              " -0.015310255,\n",
              " -0.03325391,\n",
              " 0.0049246587,\n",
              " -0.009970491,\n",
              " 0.0069147805,\n",
              " 0.049630444,\n",
              " -0.051078644,\n",
              " 0.0710505,\n",
              " -0.07212227,\n",
              " 0.014028713,\n",
              " -0.0039652484,\n",
              " -0.030573502,\n",
              " 0.035072844,\n",
              " -0.025866162,\n",
              " -0.033041764,\n",
              " 0.023931714,\n",
              " 0.014035653,\n",
              " -0.015747368,\n",
              " -0.042261735,\n",
              " 0.0028005617,\n",
              " -0.0047397106,\n",
              " -0.0016750308,\n",
              " -0.007684546,\n",
              " 0.022748126,\n",
              " -0.05401668,\n",
              " 0.022321891,\n",
              " 0.05747806,\n",
              " -0.032267727,\n",
              " -0.0018111368,\n",
              " 0.021240143,\n",
              " 0.015935048,\n",
              " -0.017525177,\n",
              " 0.035998467,\n",
              " 0.009683403,\n",
              " -0.0242088,\n",
              " 0.015660904,\n",
              " 0.048446238,\n",
              " 0.019246493,\n",
              " -0.048423514,\n",
              " 0.06908145,\n",
              " -0.049179938,\n",
              " -0.013505337,\n",
              " 0.061395,\n",
              " 0.028108947,\n",
              " -0.033591855,\n",
              " -0.038586985,\n",
              " 0.041065104,\n",
              " -0.022049988,\n",
              " -0.010130617,\n",
              " 0.038356528,\n",
              " -0.034783028,\n",
              " -0.043130342,\n",
              " -0.0005808886,\n",
              " -0.04879479,\n",
              " 0.0053223893,\n",
              " -0.0228413,\n",
              " 0.045616776,\n",
              " -0.007167411,\n",
              " -0.03421718,\n",
              " 0.0073885787,\n",
              " 0.040428832,\n",
              " 0.025090184,\n",
              " -0.024634155,\n",
              " -0.024775982,\n",
              " 0.0052057267,\n",
              " 0.05093614,\n",
              " 0.042788316,\n",
              " 0.024823798,\n",
              " -0.029019883,\n",
              " 0.03016593,\n",
              " 0.021206478,\n",
              " -0.012097347,\n",
              " 0.002654971,\n",
              " 0.029118014,\n",
              " 0.018068524,\n",
              " -0.017020179,\n",
              " -0.022816472,\n",
              " -0.063054584,\n",
              " -0.00427345,\n",
              " 0.026169498,\n",
              " 0.08122146,\n",
              " -0.048106845,\n",
              " -0.021307718,\n",
              " 0.000573938,\n",
              " -0.041324914,\n",
              " -0.033583045,\n",
              " 0.018485945,\n",
              " -0.009407308,\n",
              " 0.01742343,\n",
              " 0.0328296,\n",
              " -0.050955404,\n",
              " 0.051715422,\n",
              " -0.06056332,\n",
              " -0.058261856,\n",
              " -0.009281477,\n",
              " -0.011629536,\n",
              " -0.052582953,\n",
              " 0.0048258896,\n",
              " 0.04804673,\n",
              " -0.007417617,\n",
              " -0.006769257,\n",
              " -0.013769851,\n",
              " -0.07769279,\n",
              " 0.022878287,\n",
              " -0.009451909,\n",
              " 0.046827216,\n",
              " 0.043176863,\n",
              " 0.014981078,\n",
              " -0.012839531,\n",
              " 0.02928201,\n",
              " -0.0042603286,\n",
              " 0.014360433,\n",
              " 0.028610492,\n",
              " 0.018387672,\n",
              " -0.0038107908,\n",
              " -0.03366308,\n",
              " 0.0053267474,\n",
              " 0.0572671,\n",
              " 0.0006855626,\n",
              " -0.027601944,\n",
              " -0.07033053,\n",
              " 0.028943876,\n",
              " -0.033198193,\n",
              " 0.053086396,\n",
              " 0.07436674,\n",
              " 0.020729132,\n",
              " -0.031300083,\n",
              " 0.049042925,\n",
              " -0.01617157,\n",
              " -0.02372223,\n",
              " -0.017318686,\n",
              " 0.02420875,\n",
              " 0.00021474871,\n",
              " -0.008650192,\n",
              " 0.06548937,\n",
              " -0.034736387,\n",
              " -0.058506683,\n",
              " -0.049474478,\n",
              " -0.012243532,\n",
              " -0.013708403,\n",
              " -0.055309515,\n",
              " -0.013009869,\n",
              " -0.04248218,\n",
              " -0.02547653,\n",
              " -0.084748425,\n",
              " 0.004215639,\n",
              " 0.038781103,\n",
              " 0.06744112,\n",
              " 0.04144784,\n",
              " -0.002465784,\n",
              " -0.017555721,\n",
              " -0.005858821,\n",
              " 0.047553077,\n",
              " 0.047972098,\n",
              " -0.021811942,\n",
              " 0.019362554,\n",
              " -0.021370107,\n",
              " -0.0034753596,\n",
              " -0.08682825,\n",
              " 1.2328685e-05,\n",
              " 0.0107819205,\n",
              " -0.033175968]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(result['embedding'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LRebzTJABUh",
        "outputId": "1a53e1b2-084e-4262-8388-0d712fd063e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "\n",
        "result : Dict = genai.embed_content(\n",
        "    model=\"models/text-embedding-004\",\n",
        "    content=[\n",
        "        \"What is the meaning of life?\",\n",
        "        \"How much wood would a woodchuck chuck?\",\n",
        "        \"How does the brain work?\",\n",
        "    ],\n",
        "    task_type=\"retrieval_document\",\n",
        "    title=\"Embedding of list of strings\",\n",
        ")\n",
        "\n",
        "# A list of inputs > A list of vectors output\n",
        "for v in result[\"embedding\"]:\n",
        "    print(str(v)[:50], \"... TRIMMED ...\", len(v))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "1ZqSNYKA84oQ",
        "outputId": "d0bfa5d8-202d-4707-f1dd-aba1401f8520",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.036453027, 0.033254996, -0.03970925, -0.002628 ... TRIMMED ... 768\n",
            "[-0.01591948, 0.032582663, -0.081024624, -0.011298 ... TRIMMED ... 768\n",
            "[0.00037063024, 0.03763057, -0.122695684, -0.00951 ... TRIMMED ... 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Vector Stores & Retreival using Chroma DB"
      ],
      "metadata": {
        "id": "eyrCQBs8AANE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq langchain-chroma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPpBBX9g-PKl",
        "outputId": "25dfc088-d126-4d02-e7f5-e697129b3afa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.8/70.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.47.1 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os"
      ],
      "metadata": {
        "id": "w4Wd_L8zAJTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
        "        metadata={\"source\": \"mammal-pets-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
        "        metadata={\"source\": \"mammal-pets-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Goldfish are popular pets for beginners, requiring relatively simple care.\",\n",
        "        metadata={\"source\": \"fish-pets-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Parrots are intelligent birds capable of mimicking human speech.\",\n",
        "        metadata={\"source\": \"bird-pets-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Rabbits are social animals that need plenty of space to hop around.\",\n",
        "        metadata={\"source\": \"mammal-pets-doc\"},\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "uxHZV_umADw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -Uq langchain-google-genai google-generativeai\n"
      ],
      "metadata": {
        "id": "wrgG9s0RBRo9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "244347e9-0fc2-4b84-c166-f5c298054370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "uLq9TZU5BpZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import os\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents,\n",
        "    embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))"
      ],
      "metadata": {
        "id": "H6v2VV9lBNlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(dir(vectorstore))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4F_AqazDjvc",
        "outputId": "2c649de9-56ce-4141-c3e9-c5ced3376dad",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_Chroma__ensure_collection',\n",
              " '_Chroma__query_collection',\n",
              " '_LANGCHAIN_DEFAULT_COLLECTION_NAME',\n",
              " '__abstractmethods__',\n",
              " '__annotations__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__slots__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_abc_impl',\n",
              " '_asimilarity_search_with_relevance_scores',\n",
              " '_chroma_collection',\n",
              " '_client',\n",
              " '_client_settings',\n",
              " '_collection',\n",
              " '_collection_metadata',\n",
              " '_collection_name',\n",
              " '_cosine_relevance_score_fn',\n",
              " '_embedding_function',\n",
              " '_euclidean_relevance_score_fn',\n",
              " '_get_retriever_tags',\n",
              " '_max_inner_product_relevance_score_fn',\n",
              " '_persist_directory',\n",
              " '_select_relevance_score_fn',\n",
              " '_similarity_search_with_relevance_scores',\n",
              " 'aadd_documents',\n",
              " 'aadd_texts',\n",
              " 'add_documents',\n",
              " 'add_images',\n",
              " 'add_texts',\n",
              " 'adelete',\n",
              " 'afrom_documents',\n",
              " 'afrom_texts',\n",
              " 'aget_by_ids',\n",
              " 'amax_marginal_relevance_search',\n",
              " 'amax_marginal_relevance_search_by_vector',\n",
              " 'as_retriever',\n",
              " 'asearch',\n",
              " 'asimilarity_search',\n",
              " 'asimilarity_search_by_vector',\n",
              " 'asimilarity_search_with_relevance_scores',\n",
              " 'asimilarity_search_with_score',\n",
              " 'delete',\n",
              " 'delete_collection',\n",
              " 'embeddings',\n",
              " 'encode_image',\n",
              " 'from_documents',\n",
              " 'from_texts',\n",
              " 'get',\n",
              " 'get_by_ids',\n",
              " 'max_marginal_relevance_search',\n",
              " 'max_marginal_relevance_search_by_vector',\n",
              " 'override_relevance_score_fn',\n",
              " 'reset_collection',\n",
              " 'search',\n",
              " 'similarity_search',\n",
              " 'similarity_search_by_image',\n",
              " 'similarity_search_by_image_with_relevance_score',\n",
              " 'similarity_search_by_vector',\n",
              " 'similarity_search_by_vector_with_relevance_scores',\n",
              " 'similarity_search_with_relevance_scores',\n",
              " 'similarity_search_with_score',\n",
              " 'similarity_search_with_vectors',\n",
              " 'update_document',\n",
              " 'update_documents']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV6SYPkLCUZf",
        "outputId": "e4e2b7ef-d692-4db3-eea0-4223daeba5f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_chroma.vectorstores.Chroma at 0x7f8915e19dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore.similarity_search(\"cat\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enYsspQlCVTx",
        "outputId": "bb75f7e0-6c15-4f9d-cabb-03bf4becdeac",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='a22152a6-4f84-4f9d-a30d-95280e91c3bb', metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
              " Document(id='77d800e4-07b1-475d-8f20-536b86c85679', metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
              " Document(id='512a367f-aaf3-4550-bee9-fca5f7f95229', metadata={'source': 'mammal-pets-doc'}, page_content='Rabbits are social animals that need plenty of space to hop around.'),\n",
              " Document(id='6d4219db-4d42-4386-9842-b8e176bb12f4', metadata={'source': 'bird-pets-doc'}, page_content='Parrots are intelligent birds capable of mimicking human speech.')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await vectorstore.asimilarity_search(\"cat\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhUxC04ODcB_",
        "outputId": "17da1c89-5b0f-40d8-a662-6311df5b1c58",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='a22152a6-4f84-4f9d-a30d-95280e91c3bb', metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
              " Document(id='77d800e4-07b1-475d-8f20-536b86c85679', metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
              " Document(id='512a367f-aaf3-4550-bee9-fca5f7f95229', metadata={'source': 'mammal-pets-doc'}, page_content='Rabbits are social animals that need plenty of space to hop around.'),\n",
              " Document(id='6d4219db-4d42-4386-9842-b8e176bb12f4', metadata={'source': 'bird-pets-doc'}, page_content='Parrots are intelligent birds capable of mimicking human speech.')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that providers implement different scores; Chroma here\n",
        "# returns a distance metric that should vary inversely with\n",
        "# similarity.\n",
        "\n",
        "vectorstore.similarity_search_with_score(\"cat\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqdDbqlqD66I",
        "outputId": "fc151fa6-2d08-4dfe-c5b7-b685e29222c0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Document(id='a22152a6-4f84-4f9d-a30d-95280e91c3bb', metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
              "  0.6390674114227295),\n",
              " (Document(id='77d800e4-07b1-475d-8f20-536b86c85679', metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
              "  0.8766832947731018),\n",
              " (Document(id='512a367f-aaf3-4550-bee9-fca5f7f95229', metadata={'source': 'mammal-pets-doc'}, page_content='Rabbits are social animals that need plenty of space to hop around.'),\n",
              "  0.8958533406257629),\n",
              " (Document(id='6d4219db-4d42-4386-9842-b8e176bb12f4', metadata={'source': 'bird-pets-doc'}, page_content='Parrots are intelligent birds capable of mimicking human speech.'),\n",
              "  0.93742835521698)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\").embed_query(\"cat\")\n",
        "\n",
        "vectorstore.similarity_search_by_vector(embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5wF724BDNb5",
        "outputId": "47ba91ac-4f1d-4f5d-9a9e-96d121c1b16c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='a22152a6-4f84-4f9d-a30d-95280e91c3bb', metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
              " Document(id='77d800e4-07b1-475d-8f20-536b86c85679', metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
              " Document(id='512a367f-aaf3-4550-bee9-fca5f7f95229', metadata={'source': 'mammal-pets-doc'}, page_content='Rabbits are social animals that need plenty of space to hop around.'),\n",
              " Document(id='6d4219db-4d42-4386-9842-b8e176bb12f4', metadata={'source': 'bird-pets-doc'}, page_content='Parrots are intelligent birds capable of mimicking human speech.')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding"
      ],
      "metadata": {
        "id": "fVH5rbdBFGiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrievers"
      ],
      "metadata": {
        "id": "ZJBBUPFvFkym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "retriever = RunnableLambda(vectorstore.similarity_search).bind(k=1)  # select top result\n",
        "\n",
        "retriever.batch([\"shark\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndnSdfyoEsfz",
        "outputId": "1fc19b94-d823-429d-a303-462184b88387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Document(id='77d800e4-07b1-475d-8f20-536b86c85679', metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.')]]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq langchain-google-genai"
      ],
      "metadata": {
        "id": "KJLP_WTbGxXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",\n",
        "                             api_key = userdata.get('GOOGLE_API_KEY')\n",
        ")"
      ],
      "metadata": {
        "id": "B5Qg_8mtGExp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "message = \"\"\"\n",
        "Answer this question using the provided context only.\n",
        "\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vEIe1ORnGv5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([(\"human\", message)])"
      ],
      "metadata": {
        "id": "rKGDcuBQI63Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm\n"
      ],
      "metadata": {
        "id": "Sc2yWh5BJJuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke(\"tell me about cat\")\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFCGSNa3Jn5j",
        "outputId": "f77511d4-f50c-44f8-8f2c-d264559efb9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided text, cats are independent pets that often enjoy their own space.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now use google gemini embedding model for retriver\n",
        "https://python.langchain.com/api_reference/google_genai/embeddings/langchain_google_genai.embeddings.GoogleGenerativeAIEmbeddings.html#langchain_google_genai.embeddings.GoogleGenerativeAIEmbeddings"
      ],
      "metadata": {
        "id": "kv5FmiyvL0x_"
      }
    }
  ]
}